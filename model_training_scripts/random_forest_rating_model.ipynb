{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ce559a",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5ddcc",
   "metadata": {},
   "source": [
    "This notebook is used to train a random forest model to predict the rating of a Hostelworld review based on the sentiment of the text in the review.\n",
    "\n",
    "Possibly an ordinal regression model would perform better than a random forest but there is no package that works well to do this in Python (Note the package mord is not maintained).The below link could be worth investigating further or else find a way to manually train the model.\n",
    "https://www.statsmodels.org/dev/examples/notebooks/generated/ordinal_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152dabf7",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4803f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ba25c",
   "metadata": {},
   "source": [
    "# Read in Data and Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/message_df_labelled.csv')\n",
    "df.loc[(df['rating']==\"MIXED\")|(df['rating']==\"NEUTRAL\"), \"rating\"] = 'OTHER'\n",
    "df = df.loc[df['rating'].notnull()]\n",
    "df = pd.concat([df, pd.get_dummies(df['rating'])], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334e651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the df contains each message within a review and the sentiment of that review. We want to aggregate each review\n",
    "# so that we get a count of how many positive, negative and neutral sentences are in each review\n",
    "# we will use these number to predict the rating of a review. The rest of variables are solely to provide\n",
    "#Â information on each review incase we need it, we will exclude these when training\n",
    "df_agg = (\n",
    "df.groupby('review_id')\n",
    ".agg(\n",
    "    review_id=('review_id', min),\n",
    "    city=('city', min),\n",
    "    country=('country', min),\n",
    "    hostel_id=('hostel_id', min),\n",
    "    hostel_url=('hostel_url', min),\n",
    "    hostel_name=('hostel_name', min),\n",
    "    hostel_overall_rating=('hostel_overall_rating', min),\n",
    "    review_text=('review_text', min),\n",
    "    review_rating=('review_rating', min),\n",
    "    eviewer_nationality=('reviewer_nationality', min),\n",
    "    reviewer_gender=('reviewer_gender', min),\n",
    "    reviewer_age_group=('reviewer_age_group', min),\n",
    "    stay_date=('stay_date', min),\n",
    "    total_messages=('hostel_id', 'count'),\n",
    "    count_negative_sentences=('NEGATIVE', sum),\n",
    "    count_positive_sentences=('POSITIVE', sum),\n",
    "    count_other_sentences=('OTHER', sum)\n",
    ")\n",
    ".reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08581cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to scale the response variable for training between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "df_agg['review_rating_scaled'] = scaler.fit_transform(np.array(df_agg['review_rating']).reshape(-1, 1))\n",
    "\n",
    "# keeping some extra cols in for now so we can look through predictions better on the test set\n",
    "cols = [\n",
    "        'review_id', \n",
    "        'review_text',\n",
    "        'hostel_name',\n",
    "        'count_positive_sentences',\n",
    "        'count_negative_sentences',\n",
    "        'count_other_sentences',\n",
    "        'review_rating',\n",
    "        'review_rating_scaled'\n",
    "       ]\n",
    "df_agg = df_agg[cols]\n",
    "predictors = [x for x in df_agg.columns if x not in [\"review_rating_scaled\"]]\n",
    "X = df_agg[predictors]\n",
    "Y = df_agg[['review_rating_scaled']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ecc66d",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d01112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=222)\n",
    "x_test_original = x_test.copy() # we want this so we can use the review id to look at the test set predictions\n",
    "x_test_original['review_rating_scaled'] = Y\n",
    "\n",
    "predictors = [x for x in x_train.columns if x not in [\"review_id\", \"hostel_name\", 'review_rating', 'review_text']]\n",
    "# predictors = [x for x in x_train.columns if x in relevant_features]\n",
    "x_train = x_train[predictors]\n",
    "x_test = x_test[predictors]\n",
    "x_train_original = x_train.copy() # so we can look at feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 8)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 8)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 15, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train, y_train)\n",
    "rf_best_parameters  = rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a random forest model predicts scores between the min and max that it sees in the training set which is why it works\n",
    "# well for this task compared to some other models that may predict more or less than the scale of ratings (0-10)\n",
    "\n",
    "# regression_model = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=100)\n",
    "regression_model = RandomForestRegressor(**rf_best_parameters)\n",
    "regression_model.fit(x_train.values, y_train['review_rating_scaled'].ravel())\n",
    "y_pred  = regression_model.predict(x_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(\"R2: %f\" % (r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2431dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load the model\n",
    "dump(regression_model, 'rating_prediction_model.joblib') \n",
    "regression_model = load('rating_prediction_model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28934d95",
   "metadata": {},
   "source": [
    "# Look at Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Minimum rating in test set {y_test.min()}')\n",
    "print(f'Minimum rating in test set {y_test.max()}')\n",
    "print(f'Minimum rating in test set predictions {y_pred.min()}')\n",
    "print(f'Maximum rating in test set predictions {y_pred.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953df0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of predicted ratings\n",
    "plt.hist(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature importance for the model\n",
    "importances = regression_model.feature_importances_\n",
    "features = x_train_original.columns\n",
    "indices = np.argsort(importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation between predictions and actuals with a scatter plot\n",
    "plt.scatter(y_test, y_pred, c =\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_original['pred'] = y_pred\n",
    "x_test_original"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
